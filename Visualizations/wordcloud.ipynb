{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (<ipython-input-9-fe5236733de1>, line 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-fe5236733de1>\"\u001b[0;36m, line \u001b[0;32m40\u001b[0m\n\u001b[0;31m    clean_word = ''.join([char for char in to_be_clean_word if char in validLetters])\u001b[0m\n\u001b[0m                                                                                     ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Importing required libraries\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "# Connecting to the webpage and parsing the html file\n",
    "response = urllib2.urlopen('http://www.nytimes.com/')\n",
    "content = response.read()\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n",
    "# Initializing the list to hold all the words in the headlines\n",
    "headlines_words = []\n",
    "print ('\\n--- Printing the headlines...')\n",
    "# Looping through all healine sections of the page found by beautiful soup\n",
    "for story_heading in soup.find_all(class_=\"story-heading\"):\n",
    "\t\n",
    "\tstory_title = story_heading.text.replace(\"\\n\",\"\").strip().encode('utf-8')\n",
    "\tif story_title != '':\n",
    "\t print ('\\n', story_title)\n",
    "\t\n",
    "\t# Save a lower-case version of each word to my list\n",
    "for word in story_title.strip().split():\n",
    "    headlines_words.append(word.lower())\n",
    "# Creating a new list for the dropwords and for the filtered words\n",
    "stopwords = []\n",
    "filtered_words = []\n",
    "# Creating a list of stopwords from text file\n",
    "stopwordsfile = open('stopwords_en.txt','r')\n",
    "for line in stopwordsfile:\n",
    "\tstopwords.append(line.strip())\n",
    "# Adding words not relevant for this analysis to the stopwords list\n",
    "stopwords.extend(('', 'trump', 'new', 'good', 'make', 'just', 'home', 'pm', 'et'))\n",
    "# Creating a list of character/letters allowed in the words\n",
    "validLetters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "# Creating the list of words by removing\n",
    "#   the stopword and words with characters not in the list of allowed characters\n",
    "for to_be_clean_word in headlines_words:\n",
    "\tif to_be_clean_word not in stopwords:\n",
    "    \tclean_word = ''.join([char for char in to_be_clean_word if char in validLetters])\n",
    "    \tfiltered_words.append(clean_word.lower())\n",
    "print ('\\n--- Extracting bigrams from the text...')\n",
    "\n",
    "# Extracting the bigrams\n",
    "bigrams_list = [] # Initializing the list of bigrams\n",
    "for i in range(len(filtered_words)):\n",
    "\ttry:\n",
    "    \tbigrams_list.append( filtered_words[i] + \"_\" + filtered_words[i+1] )\n",
    "\texcept:\n",
    "    \tpass # reached end of list\n",
    "# Extracting the 7 most common bigrams, with their occurrence\n",
    "common_bigrams_tuples = Counter(bigrams_list).most_common(7)\n",
    "# Creating a list of the 7 most common bigrams (with no occurrences).\n",
    "#   To keep the ouccurrence of the bigrams, each one is multiplied by its own occurrence.\n",
    "#   This is relevant to give them the proper exposure in the wordcloud\n",
    "common_bigrams = []\n",
    "for bigram_tuple_count in range (0, len(common_bigrams_tuples)):\n",
    "\tmultiple_bigram = (common_bigrams_tuples[bigram_tuple_count][0] + ' ') * int(common_bigrams_tuples[bigram_tuple_count][1])\n",
    "\tcommon_bigrams.append(multiple_bigram)\n",
    "# Appending the list of most common bigrams to the list of words, generating the list for the wordcloud\n",
    "#   and then transforming the list into a string\n",
    "filtered_words.extend(common_bigrams)\n",
    "wordcloud_str = ' '.join(filtered_words)\n",
    "print ('\\n--- Generating the wordcloud from the text...')\n",
    "# Generating Wordcloud and its file\n",
    "wc = WordCloud(background_color=\"white\", max_words=4000)\n",
    "wc.generate(wordcloud_str)\n",
    "wc.to_file('NYT.png')\n",
    "# Showing the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print ('\\n--- END OF PROCESSING ---\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
