{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Printing the headlines...\n"
     ]
    }
   ],
   "source": [
    "#LInking website/parsing the html file\n",
    "responses = urllib2.urlopen('http://www.nytimes.com/')\n",
    "info = response.read()\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "#all the words in the headline\n",
    "headlines_words = []\n",
    "print '\\n Printing the headlines...'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop for all healine sections by beautiful soup\n",
    "for heading in soup.find_all(class_=\"story-heading\"):\n",
    "    \n",
    "    title_of_story = heading.text.replace(\"\\n\",\"\").strip().encode('utf-8')\n",
    "    if title_of_story != '':\n",
    "    \tprint '\\n', title_of_story\n",
    "    \n",
    "#saving lowercase\n",
    "    for word in title_of_story.strip().split(): \n",
    "        headlines_words.append(word.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new list for the dropwords and for the filtered words \n",
    "stopwordstext = []\n",
    "filtered = []\n",
    "\n",
    "# Creating a list of stopwords from text file\n",
    "stopwordsfile = open('stopwords_en.txt','r')\n",
    "for line in stopwordsfile:\n",
    "    stopwords.append(line.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Extracting bigrams from the text...\n"
     ]
    }
   ],
   "source": [
    "# Adding words not relevant for this analysis to the stopwords list\n",
    "stopwordstext.extend(('', 'trump', 'new', 'good', 'make', 'just', 'home', 'pm', 'et'))\n",
    "\n",
    "# Creating a list of character/letters allowed in the words\n",
    "letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "\n",
    "# Creating the list of words by removing\n",
    "#   the stopword and words with characters not in the list of allowed characters\n",
    "for to_be_clean_word in headlines_words:\n",
    "    if to_be_clean_word not in stopwords:\n",
    "        clean_word = ''.join([char for char in to_be_clean_word if char in letters])\n",
    "        filtered.append(clean_word.lower())\n",
    "\n",
    "print '\\n Extracting bigrams from the text...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the bigrams\n",
    "bigrams = [] # Initializing the list of bigrams\n",
    "for i in range(len(filtered)):\n",
    "    try:\n",
    "        bigrams.append( filtered[i] + \"_\" + filtered[i+1] )\n",
    "    except:\n",
    "        pass # reached end of list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating the wordcloud from the text...\n"
     ]
    }
   ],
   "source": [
    "# Extracting the 7 most common bigrams, with their occurrence\n",
    "common_bigrams_tuples = Counter(bigrams_list).most_common(7)\n",
    "\n",
    "# Creating a list of the 7 most common bigrams (with no occurrences).\n",
    "#   To keep the ouccurrence of the bigrams, each one is multiplied by its own occurrence.\n",
    "#   This is relevant to give them the proper exposure in the wordcloud\n",
    "common_bigrams = []\n",
    "for bigram_tuple_count in range (0, len(common_bigrams_tuples)):\n",
    "    multiple_bigram = (common_bigrams_tuples[bigram_tuple_count][0] + ' ') * int(common_bigrams_tuples[bigram_tuple_count][1])\n",
    "    common_bigrams.append(multiple_bigram)\n",
    "\n",
    "# Appending the list of most common bigrams to the list of words, generating the list for the wordcloud\n",
    "#   and then transforming the list into a string\n",
    "filtered.extend(common_bigrams)\n",
    "wordcloud_str = ' '.join(filtered)\n",
    "\n",
    "print '\\n--- Generating the wordcloud from the text...'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "We need at least 1 word to plot a word cloud, got 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-af2bfac95b99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mwc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackground_color\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"white\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m800\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m800\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mwc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordcloud_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mwc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NewYorkTimes.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Stevens User\\Anaconda2\\lib\\site-packages\\wordcloud\\wordcloud.pyc\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    617\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m         \"\"\"\n\u001b[1;32m--> 619\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    620\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Stevens User\\Anaconda2\\lib\\site-packages\\wordcloud\\wordcloud.pyc\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    599\u001b[0m         \"\"\"\n\u001b[0;32m    600\u001b[0m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Stevens User\\Anaconda2\\lib\\site-packages\\wordcloud\\wordcloud.pyc\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m             raise ValueError(\"We need at least 1 word to plot a word cloud, \"\n\u001b[1;32m--> 391\u001b[1;33m                              \"got %d.\" % len(frequencies))\n\u001b[0m\u001b[0;32m    392\u001b[0m         \u001b[0mfrequencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfrequencies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: We need at least 1 word to plot a word cloud, got 0."
     ]
    }
   ],
   "source": [
    "# Generating Wordcloud and its file\n",
    "\n",
    "wc = WordCloud(background_color=\"white\", max_words=200, width=800, height=800)\n",
    "wc.generate(wordcloud_str)\n",
    "wc.to_file('NewYorkTimes.png')\n",
    "\n",
    "#https://www.programcreek.com/python/example/103472/wordcloud.WordCloud\n",
    "\n",
    "# Showing the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print '\\n--- END OF PROCESSING ---\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
